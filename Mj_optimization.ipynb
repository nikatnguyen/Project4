{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#import deployment dependencies\n",
    "import pickle\n",
    "\n",
    "#  Import and read the clean_df.csv.\n",
    "import pandas as pd \n",
    "obesity_df = pd.read_csv('https://raw.githubusercontent.com/nikatnguyen/Project4/main/Resources/clean_df.csv')\n",
    "obesity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Age column to integers to remove decimals\n",
    "obesity_df['Age'] = obesity_df['Age'].astype(int)\n",
    "\n",
    "# Convert the Weight column to integers to remove decimals\n",
    "obesity_df['Weight'] = obesity_df['Weight'].astype(int)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(obesity_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into features and targets\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "X = obesity_df.drop(columns=['Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'])\n",
    "y = obesity_df[['Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']].values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MJ ATTEMPT 1\n",
    "nn1 = tf.keras.models.Sequential()\n",
    "nn1.add(tf.keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "nn1.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "nn1.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "fit_model = nn1.fit(X_train_scaled, y_train, epochs=100)\n",
    "\n",
    "# Check the structure of the model\n",
    "nn1.summary()\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn1.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MJ ATTEMPT 2\n",
    "nn2 = tf.keras.models.Sequential()\n",
    "nn2.add(tf.keras.layers.Dense(units=15, activation=\"relu\"))\n",
    "nn2.add(tf.keras.layers.Dense(units=10, activation=\"relu\"))\n",
    "nn2.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "fit_model = nn2.fit(X_train_scaled, y_train, epochs=100)\n",
    "\n",
    "# Check the structure of the model\n",
    "nn2.summary()\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn2.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MJ ATTEMPT 3\n",
    "nn3 = tf.keras.models.Sequential()\n",
    "nn3.add(tf.keras.layers.Dense(units=15, activation=\"relu\"))\n",
    "nn3.add(tf.keras.layers.Dense(units=15, activation=\"relu\"))\n",
    "nn3.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "fit_model = nn3.fit(X_train_scaled, y_train, epochs=100)\n",
    "\n",
    "# Check the structure of the model\n",
    "nn3.summary()\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn3.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
